{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "486225e8-12a5-499f-a620-ea41ccef1815",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation and Chatbot Application\n",
    "\n",
    "Many use cases such as building a chatbot require text (text2text) generation models to respond to user questions with insightful answers. The leading LLM models have picked up a lot of general knowledge in training, but we often need to ingest and use a large library of more specific information.\n",
    "\n",
    "In this notebook we will demonstrate how to use a LLM to answer questions using a library of documents as a reference, by using document embeddings and retrieval. The embeddings are generated from Huggingface embedding model. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25664980",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Quick introduction to LangChain and why is it useful for RAG based applications?\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "\n",
    "**LLMs are powerful by themselves. Why do we need libraries like LangChain?**\n",
    "\n",
    "While LLMs are powerful, they are also general in nature (& thus kinda boring & limited). While LLMs can perform many tasks effectively, they are not able to provide specific answers to questions or tasks that require deep domain knowledge or expertise. For example, imagine you want to use an LLM to answer questions about a specific field, like medicine or law. While the LLM may be able to answer general questions about the field, it may not be able to provide more detailed or nuanced answers that require specialized knowledge or expertise. To work around this limitation, LangChain offers a useful approach where the corpus of text is preprocessed by breaking it down into chunks or summaries, embedding them in a vector space, and searching for similar chunks when a question is asked. LangChain also provides a level of abstraction, making it super easy to use. LangChain's popularity has grown exponentially since it was first introduced and being an open source library, it is constantly evolving!\n",
    "\n",
    "**How do we solve for this problem?**\n",
    "\n",
    "RAG - Retrieval Augmented Generation. Is a design pattern that enterprise customers could leverage to bring domain context and their artifacts (securely) while leveraging LLMs to answer questions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7de785d0-3b27-4699-87be-a34484c429fa",
   "metadata": {},
   "source": [
    "### 1.1 Key components of LangChain\n",
    "\n",
    "Let us examine the key components of Langchain. These components are, in increasing order of complexity:\n",
    "\n",
    "#### Models\n",
    "\n",
    "Building blocks to interface with any language model. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs, including integration with LLM hosted on SageMaker Endpoint.\n",
    "\n",
    "<img src='./images/models.png' max-width =\"1080\"/>\n",
    "\n",
    "    \n",
    "#### Prompts\n",
    "\n",
    "The new way of programming models is through prompts. A \"prompt\" refers to the input to the model. This input is rarely hard coded, but rather is often constructed from multiple components. A PromptTemplate is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy, such as:\n",
    "\n",
    "- Prompt templates: Parameterized model inputs\n",
    "- Example selectors: Dynamically select examples to include in prompts\n",
    "    \n",
    "<img src=\"images/prompt.png\" max-width=\"1080\"/>\n",
    "\n",
    "#### Indexes\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set. Indexes refer to ways to structure documents so that LLMs can best interact with them. LangChain gives you the building blocks to load, transform, store and query your data via:\n",
    "\n",
    "- Document loaders: Load documents from many different sources\n",
    "- Document transformers: Split documents, convert documents into Q&A format, drop redundant documents, and more\n",
    "- Text embedding models: Take unstructured text and turn it into a list of floating point numbers\n",
    "- Vector stores: Store and search over embedded data\n",
    "- Retrievers: Query your data\n",
    "        \n",
    "The primary index and retrieval types supported by LangChain are currently centered around vector databases, but it can also interact with structured data (SQL tables, etc) or external APIs.\n",
    "\n",
    "<img src=\"images/vectorstore.png\" max-width=\"1080\"/>\n",
    "\n",
    "#### Memory\n",
    "\n",
    "Most LLM applications have a conversational interface. Memory is the concept of storing and retrieving data in the process of a conversation. There are two main methods:\n",
    "\n",
    "#### Chains\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "LangChain provides the Chain interface for such \"chained\" applications. LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. \n",
    "\n",
    "We define a Chain very generically as a sequence of calls to components, which can include other chains. Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\n",
    "\n",
    "<img src=\"images/chains.png\" max-width=\"1080\"/>\n",
    "\n",
    "#### Agents\n",
    "\n",
    "The core idea of agents is to use an LLM to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "    \n",
    "#### Callbacks\n",
    "\n",
    "It can be difficult to track all that occurs inside a chain or agent. Callbacks help add a level of observability and introspection.\n",
    " \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "402714bf-14b6-4481-8e33-fc3d0b8a81f4",
   "metadata": {},
   "source": [
    "### 1.2 Chat Bot key elements\n",
    "\n",
    "The first process in a chat bot is to generate embeddings. Typically you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using a GPT-J embeddings model for this\n",
    "\n",
    "<img src=\"images/Embeddings_lang.png\" max-width=\"1080\"/>\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "<img src=\"images/Chatbot_lang.png\" max-width=\"1080\"/>\n",
    "\n",
    "For processes which need deeper analysis, conversation history we will need to summarize every interaction to keep it succinct and for that we can follow this flow below which uses PineCone as an example for the various Tools which are available \n",
    "\n",
    "<img src=\"images/chatbot_internet.jpg\" width=\"1080\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f1176e9-9a60-4713-b72f-9e54d2a259b8",
   "metadata": {},
   "source": [
    "### 1.3 Key points for consideration\n",
    "\n",
    "1. If we have long Document that exceed the LLM token limit, consider using Chains interface to process documents: Map Reduce, Refine, Map-Rerank\n",
    "2. To optimize cost per token -- minimize the tokens and send in only relevant tokens to Model\n",
    "3. Which model to use --\n",
    "    - Cohere, AI21, Huggingface Hub, Manifest, Goose AI, Writer, Banana, Modal, StochasticAI, Cerebrium, Petals, Forefront AI, Anthropic, DeepInfra, and self-hosted Models.\n",
    "    - Example LLM Cohere = Cohere(model='command-xlarge')\n",
    "    - Example LLM Flan = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")\n",
    "4. Input data sources could be PDF, WebPages, CSV, S3, EFS\n",
    "5. Orchestration with external tasks\n",
    "    - External tasks - Agent SerpApi, SEARCH Engines\n",
    "    - Math calculator\n",
    "6. Conversation management and history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aed6880-101b-457a-9e99-25cc421ee8c5",
   "metadata": {},
   "source": [
    "## 2. Pre-Requisites\n",
    "\n",
    "There are a few pre-reqs to be completed when running this notebook. The key one being setting up the LLM to be used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa11828a-243d-4808-9c92-e8caf4cebd37",
   "metadata": {},
   "source": [
    "### 2.1 Install certain libraries which are needed for this run. \n",
    "\n",
    "These are provided in the requirements.txt or you can run these cells to fine control which libraries you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ec361-a696-4897-81f4-e83e3d724e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12130b4c-6242-4b81-866c-a1b8de3c577c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install wkhtmltopdf -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900e8c1-9f2d-4b14-9904-481a0ac5442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8923e9-69f8-4561-8df3-8eca59c965fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install chromadb==0.3.21 --quiet\n",
    "!pip install langchain==0.0.161 boto3 html2text jinja2 --quiet\n",
    "!pip install faiss-cpu==1.7.4 --quiet\n",
    "!pip install pypdf==3.8.1 --quiet\n",
    "!pip install transformers==4.24.0 --quiet\n",
    "!pip install sentence_transformers==2.2.2\n",
    "print(\"all libraries installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2064f1-3cfa-4f19-b6cd-e14c7f16ec1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sentence_transformers \n",
    "sentence_transformers.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f3a19c-78d7-4e60-9ba0-cec3c856ad1a",
   "metadata": {},
   "source": [
    "### 2.2 Import statements for our chain and indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a816c-b828-4d6d-9bc1-ecd0c29ddc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from aws_langchain.kendra_index_retriever import KendraIndexRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "from typing import Any, Dict, List, Optional\n",
    "import jinja2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67664670-35af-4561-a8af-1eb5967bd382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18101583-5b29-4f1e-8d41-8777472f4d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375416f1-819f-42e8-be69-b25e7e2f880b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"LLM_ENDPOINT\"]=endpoint_name\n",
    "os.environ[\"REGION\"]='us-west-2' # change this if needed\n",
    "print(os.environ[\"LLM_ENDPOINT\"])\n",
    "print(os.environ[\"REGION\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4789df3c-aff9-4957-a32d-80086b1f7ddb",
   "metadata": {},
   "source": [
    "## 3. Topics covered in this lab:\n",
    "\n",
    "In this notebook we will be covering the below topics:\n",
    "\n",
    "1. **LLM** &#8594; Examine asking an LLM without providing context\n",
    "1. **Prompt Engineering** &#8594; Improving the answer by providing insightful context\n",
    "1. **RAG based approach** &#8594; Use vector DB and prompt template to build question answering application with Retrieval Augmented Generation (RAG) approach\n",
    "1. **Chatbot** &#8594; Build a Interactive Chatbot with Memory "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf7b5d9d-154a-48bb-98e5-1b7019bb9b11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 LLM\n",
    "\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond. \n",
    "\n",
    "Make sure that you have ran the Notebook `1_deploy-falcon.ipynb` and deploy Falcon-7B model to SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdec15-11fb-4a83-b59d-b6438c9bfec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are hyper-parameters; Hyperparameters are used before inferencing a model because they have a\n",
    "# direct impact on the performance of the resulting machine learning model. \n",
    "# Hyperparameters are used before inferencing a model because they control the behavior of the model, \n",
    "# and optimize its performance for the job at hand.\n",
    "# For this workshop, hyper parameters have been identified for you. \n",
    "# If you like, you can use some of these in the code below.\n",
    "# They will impact the behavior of your LLM response. \n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": True,\n",
    "    \"temperature\": 0.2,\n",
    "    \"stop\": ['\\n'],\n",
    "    \"return_full_text\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ee7d5-f46c-4158-b9c9-8a290f32f642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_kwargs = {}\n",
    "session = boto3.Session()\n",
    "\n",
    "boto3_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "print(boto3_sm_client)\n",
    "\n",
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "prompt = f\"Answer this question below, {question}\"\n",
    "print(f\"Question being asked is -- > {prompt}:\")\n",
    "\n",
    "payload = {\"inputs\": prompt, \"parameters\": parameters}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "boto3_sm_client.invoke_endpoint(\n",
    "    EndpointName=os.environ[\"LLM_ENDPOINT\"],\n",
    "    Body=payload,\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3628a01f",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Engineering\n",
    "\n",
    "To better answer the question well, we provide extra contextual information, combine it with a prompt, and send it to model together with the question. Below is an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "print(f\"Question being asked is -- > {prompt}\")\n",
    "\n",
    "payload = {\"inputs\": prompt, \"parameters\": parameters}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "boto3_sm_client.invoke_endpoint(\n",
    "    EndpointName=os.environ[\"LLM_ENDPOINT\"],\n",
    "    Body=payload,\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33532a7c",
   "metadata": {},
   "source": [
    "The output is already significantly better than asking without providing any context. \n",
    "\n",
    "Now, the question becomes where can I find the insightful context based on the user query? The answer is to use a pre-stored knowledge data base with retrieval augmented generation, as shown below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d711c743-3d72-4c46-bcb4-6870f1d78c5e",
   "metadata": {},
   "source": [
    "### 3.3 RAG\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "1. Generate embedings for each of document in the knowledge library with HuggingFace embedding model.\n",
    "2. Identify top K most relevant documents based on user query.\n",
    "    - 2.1 For a query of your interest, generate the embedding of the query using the same embedding model.\n",
    "    - 2.2 Search the indexes of top K most relevant documents in the embedding space using in-memory Faiss search.\n",
    "    - 2.3 Use the indexes to retrieve the corresponded documents.\n",
    "3. Combine the retrieved documents with prompt and question and send them into SageMaker LLM.\n",
    "\n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt.\n",
    "\n",
    "<img src='./images/rag.jpg' max-width =\"1080\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d97ce37",
   "metadata": {},
   "source": [
    "First, let's prepare by wrapping up our LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e13020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.sagemaker_endpoint import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import ast\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=os.environ[\"LLM_ENDPOINT\"],\n",
    "    region_name=os.environ[\"REGION\"],\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "print(f\"SageMaker LLM created at {sm_llm}::\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea290ab6",
   "metadata": {},
   "source": [
    "Now, let's download the example data and prepare it for demonstration. We will use [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/) as knowledge library. The data are formatted in a CSV file with two columns Question and Answer. We use the Answer column as the documents of knowledge library, from which relevant documents are retrieved based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2b889-d6bd-4117-ad56-5dd5ef13d1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_data = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/\"\n",
    "\n",
    "!mkdir -p rag_data\n",
    "!aws s3 cp --recursive $original_data rag_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c41ed7a1",
   "metadata": {},
   "source": [
    "For the case when you have data saved in multiple subsets. The following code will read all files that end with .csv and concatenate them together. Please ensure each csv file has the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d18454-8d28-4662-a396-fb5f70730b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "all_files = glob.glob(os.path.join(\"rag_data/\", \"*.csv\"))\n",
    "\n",
    "df_knowledge = pd.concat(\n",
    "    (pd.read_csv(f, header=None, names=[\"Question\", \"Answer\"]) for f in all_files),\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "df_knowledge.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cb4751e",
   "metadata": {},
   "source": [
    "Drop the `Question` column as it is not used in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2163667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knowledge.drop([\"Question\"], axis=1, inplace=True)\n",
    "df_knowledge.to_csv(\"rag_data/processed.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eb7b83d",
   "metadata": {},
   "source": [
    "Use langchain to read the csv data. There are multiple built-in functions in LangChain to read different format of files such as txt, html, and pdf. For details, see [LangChain document loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471151c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"rag_data/processed.csv\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb76f2c",
   "metadata": {},
   "source": [
    "Next, let's generate embeddings for our docs, and store it in a LangChain VectorStore. Embeddings are a way to represent words, phrases or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them. We'll start with initializing a HuggingFace Embeddings Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbf400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Huggingface Embeddings Model\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b44a5792",
   "metadata": {},
   "source": [
    "We will store and match the embeddings using the VectorStore indexer. In this notebook, we will showcase [FAISS](https://github.com/facebookresearch/faiss) which will be transient and in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189dbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=hf_embeddings,\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=300, chunk_overlap=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a464253",
   "metadata": {},
   "source": [
    "Now it's easy to pull context from our data stores to answer prompt. We can simply use the query method on the created index and pass the user’s question and SageMaker endpoint LLM. LangChain selects the top four closest documents (K=4) and passes the relevant context extracted from the documents to generate an accurate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "index.query(question=question, llm=sm_llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43f3b4af",
   "metadata": {},
   "source": [
    "The response looks more accurate compared to the response we got with other approaches that we demonstrated earlier that have no context or static context that may not be always relevant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5573edcc",
   "metadata": {},
   "source": [
    "#### RAG alternative approach\n",
    "\n",
    "Alternatively, we can do manual approach that will let us do more customization. This approach offers the flexibility to configure top K parameters for a relevancy search in the documents. It also allows you to use the LangChain feature of [prompt templates](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html), which allow you to easily parameterize the prompt creation instead of hard coding the prompts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac8563c8",
   "metadata": {},
   "source": [
    "First, we generate embedings for each of document in the knowledge library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(documents, hf_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb9ed511",
   "metadata": {},
   "source": [
    "Based on the question above, we then identify top K most relevant documents based on user query, where K = 3 in this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "docs = docsearch.similarity_search(question, k=3)\n",
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7819484",
   "metadata": {},
   "source": [
    "Finally, we use a prompt template and chain it with the SageMaker LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112241f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"{context}\\n\\nGiven the above context, answer the following question:\\n{question}\\nAnswer: \"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain = load_qa_chain(llm=sm_llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)[\n",
    "    \"output_text\"\n",
    "]\n",
    "\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3e50531",
   "metadata": {},
   "source": [
    "With this approach of RAG implementation, we were able to take advantage of the additional flexibility of LangChain prompt templates and customize the number of documents searched for a relevancy match using the top K hyperparameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "867caa3d",
   "metadata": {},
   "source": [
    "### 3.4 Chatbot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30b0c3b9",
   "metadata": {},
   "source": [
    "Chatbots needs to remember the previous interactions. Conversational memory allows us to do that. There are several ways that we can implement conversational memory.  In the context of LangChain, they are all built on top of the ConversationChain.\n",
    "\n",
    "Let's start with learning how to make a simple chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=sm_llm, memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f36f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"What is Indonesia?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c6d6acb",
   "metadata": {},
   "source": [
    "The model has responded, now let's ask follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"What is it famous for?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff4d09da",
   "metadata": {},
   "source": [
    "We can see that the model can understand the previous conversation. Now you can clear the memory if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c976810",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe3c84b",
   "metadata": {},
   "source": [
    "Now let's build up on QA with RAG capability that we've done before.\n",
    "\n",
    "For our chatbot, we will use [ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html) chain to take in chat history (a list of messages) and new questions, and then returns an answer to that question based on retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    _template = \"\"\"{chat_history}\n",
    "\n",
    "Answer only with the new question.\n",
    "How would you ask the question considering the previous conversation: {question}\n",
    "Question:\"\"\"\n",
    "    CONVO_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "    return CONVO_QUESTION_PROMPT\n",
    "\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b313b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for ConversationRetrievalChain\n",
    "\n",
    "# retriever: We used VectoreStoreRetriver, which is backed by a VectorStore. To retrieve text, there are two search types you can choose: search_type: “similarity” or “mmr”. search_type=\"similarity\" uses similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector.\n",
    "\n",
    "# memory: Memory Chain to store the history\n",
    "\n",
    "# condense_question_prompt: Given a question from the user, we use the previous conversation and that question to make up a standalone question\n",
    "\n",
    "# chain_type: If the chat history is long and doesn't fit the context you use this parameter and the options are \"stuff\", \"refine\", \"map_reduce\", \"map-rerank\". Look up at docs for LangChain Chains Documents to learn the difference\n",
    "\n",
    "# verbose: Set to true to see the full logs and documents \n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=sm_llm, \n",
    "    retriever=docsearch.as_retriever(), \n",
    "    #retriever=docsearch.as_retriever(search_type='similarity', search_kwargs={\"k\": 8}),\n",
    "    memory=memory_chain,\n",
    "    #verbose=True,\n",
    "    #condense_question_prompt=CONDENSE_QUESTION_PROMPT, # create_prompt_template(), \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=100\n",
    ")\n",
    "\n",
    "qa.combine_docs_chain.llm_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "{context}\n",
    "\n",
    "Use at maximum 3 sentences to answer the question inside the <q></q> XML tags. \n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "Do not use any XML tags in the answer. If the answer is not in the context say \"Sorry, I don't know, as the answer was not found in the context.\"\n",
    "\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12ad24b4",
   "metadata": {},
   "source": [
    "Now let's build a utility interface for our chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4856781",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet \"ipywidgets>=7,<8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a34463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        result = self.qa.run({'question': prompt })\n",
    "                    else:\n",
    "                        result = self.qa.run({'input': prompt }) #, 'history':chat_history})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fca4e22a",
   "metadata": {},
   "source": [
    "Run our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffe03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX(qa, retrievalChain=True)\n",
    "chat.start_chat()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
